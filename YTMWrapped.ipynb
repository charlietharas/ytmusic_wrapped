{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "tRGcRBc28Zpi",
        "FTwbhcCw8gj5"
      ],
      "authorship_tag": "ABX9TyPm+TOh2nSYM5xdPpjotGls"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DO NOT BLINDLY RUN ALL IN THIS NOTEBOOK!!**"
      ],
      "metadata": {
        "id": "k73igQ9dMh-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo\n",
        "ANALYSIS\n",
        "- add `sessions()` in `Analyzer` for longest listening session\n",
        "- mark breakdown of songs listened to by replays (e.g. top 10 songs make up x % of listens)\n",
        "\n",
        "NEW FEATURES\n",
        "- matplotlib & GUI libraries for data vis (until more aesthetic things can be found) (**WIP**)\n",
        "- better skip detection via timestamps of listen & duration of song (in Loader)\n",
        "- option to store/load database when loading for API call optimization\n",
        "- add musicbrainz (or another) API to get genre and decade and album and other really good song data (**WIP**)\n",
        "- analyze user playlists (this will be a little tougher)\n",
        "- bring back proper pretty generated reports\n",
        "\n",
        "GENERAL USABILITY\n",
        "- fix loading and saving of report files (make everything automatic)\n",
        "- fix `merge_jsons()` so that it works in all cases\n",
        "- move imports to necessary sections\n",
        "- fix pandas indexing to make run faster, generally improve usage of pandas\n",
        "- add basic info and `input()`s to help any user generate their report (**make notebook that any person on the internet can use!**)\n",
        "\n",
        "LONG TERM\n",
        "- understand songs' relation to one another by their sequence (start doing ML using song metadata, essentially)"
      ],
      "metadata": {
        "id": "RXRnkI-DXCJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for the ENTIRE program (not all are necessary for each individual section)\n",
        "import datetime\n",
        "import sys\n",
        "import getopt\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import collections\n",
        "import os\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pytz\n",
        "import itertools\n",
        "import numpy as np\n",
        "import urllib.parse\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "import ast\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "ge083-6rStL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# force remount if drive connection is broken/not syncing\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ptEQU3aVJZFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loader class"
      ],
      "metadata": {
        "id": "iq7_lIIX8Xe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loader():\n",
        "    def __init__(self, use_duration, analyze_years, apikey, filepath, loadfp, ignores={\"Title\": [], \"Artist\": [], \"URL\": []}):\n",
        "        self.use_duration, self.analyze_years, self.apikey, self.filepath, self.loadfp, self.ignores = use_duration, analyze_years, apikey, filepath, loadfp, ignores\n",
        "        self.load_reports = self.filepath == \"\"\n",
        "        self.use_loadfp = self.loadfp != \"\"\n",
        "        if not self.load_reports:\n",
        "            self.file = self.open_file(self.filepath)\n",
        "        else:\n",
        "            self.file = None\n",
        "        self.out = display(Loader.progress(0, 100), display_id=True)\n",
        "\n",
        "    # utility methods\n",
        "    def should_not_ignore(self, obj):\n",
        "\n",
        "        if 'subtitles' in obj and obj['header'] == \"YouTube Music\" and obj['title'][:7] == \"Watched\" and obj['time'][:4] in self.analyze_years:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def open_file(filepath):\n",
        "        try:\n",
        "            file = open(filepath, \"r\", encoding=\"utf8\")\n",
        "            return file\n",
        "        except:\n",
        "            print(\"There was an error opening your report files\")\n",
        "\n",
        "    # takes two watch history files and merges them together (useful for takeout files which have limited date range)\n",
        "    # only works if original jsons are already sorted by date and the first argument is chronologically ahead of the second argument and the two jsons overlap\n",
        "    @staticmethod\n",
        "    def merge_jsons(file1path, file2path, outputfilepath):\n",
        "        print(\"WARN: merge_jsons() was called. Be sure that its criteria are met before using the output of this method!\")\n",
        "        file2 = Loader.open_file(file2path)\n",
        "        json2 = json.load(file2)\n",
        "        date, datestr = \"\", \"\"\n",
        "        for obj in json2:\n",
        "            if 'subtitles' in obj:\n",
        "                datestr = obj['time'][:-5]\n",
        "                date = datetime.datetime.strptime(datestr, \"%Y-%m-%dT%H:%M:%S\")\n",
        "                break\n",
        "\n",
        "        combined = \"\"\n",
        "        file1 = Loader.open_file(file1path)\n",
        "        file2 = Loader.open_file(file2path)\n",
        "        for i in file1.readlines():\n",
        "            combined += i\n",
        "            if datestr in i:\n",
        "                combined+=\"\\\"merged\\\":\\\"here\\\"},\"\n",
        "                break\n",
        "\n",
        "        for c, i in enumerate(file2.readlines()):\n",
        "            if c == 0:\n",
        "                i = i[1:]\n",
        "            combined += i\n",
        "\n",
        "        new_file = open(outputfilepath, \"wt\")\n",
        "        new_file.write(combined)\n",
        "        new_file.close()\n",
        "\n",
        "    # processes bulk lists\n",
        "    def parse_json(self):\n",
        "        self.history = {\"Title\": [], \"Artist\": [], \"Time\": [], \"URL\": [], \"Duration\": []}\n",
        "        json_object = json.load(self.file)\n",
        "        for obj in json_object:\n",
        "            if (self.should_not_ignore(obj)):\n",
        "                self.history[\"Title\"].append(obj['title'][8:])\n",
        "                self.history[\"Artist\"].append(obj['subtitles'][0]['name'].replace('- Topic ', '').replace('- Topic', ''))\n",
        "                zind = obj['time'].index(\".\")-1 if \".\" in obj['time'] else len(obj['time'])-1\n",
        "                self.history[\"Time\"].append(datetime.datetime.strptime(obj['time'][:zind], \"%Y-%m-%dT%H:%M:%S\"))\n",
        "                self.history[\"URL\"].append(obj['titleUrl'][obj['titleUrl'].index('v=')+2:])\n",
        "                self.history[\"Duration\"].append(0)\n",
        "\n",
        "        # preprocessing\n",
        "        for i in range(len(self.history[\"Title\"])):\n",
        "            if i >= len(self.history[\"Title\"]):\n",
        "                break # contingency\n",
        "            if self.history[\"Title\"][i] in self.ignores[\"Title\"] or self.history[\"Artist\"][i] in self.ignores[\"Artist\"] or self.history[\"URL\"][i] in self.ignores[\"URL\"]:\n",
        "                del self.history[\"Title\"][i]\n",
        "                del self.history[\"Artist\"][i]\n",
        "                del self.history[\"Time\"][i]\n",
        "                del self.history[\"URL\"][i]\n",
        "                del self.history[\"Duration\"][i]\n",
        "                i-= 1\n",
        "\n",
        "        occurrences = collections.Counter(self.history['URL'])\n",
        "        self.history['Occurrences'] = []\n",
        "        for i in self.history['URL']:\n",
        "            self.history['Occurrences'].append(occurrences[i])\n",
        "\n",
        "        occurrences = collections.Counter(self.history['Artist'])\n",
        "        duration = [0]*len(occurrences.keys())\n",
        "        self.artists = {\"Artist\": occurrences.keys(), \"Occurrences\": occurrences.values(), \"Duration\": duration}\n",
        "\n",
        "    # generates dataframes and csv files\n",
        "    def gen_dataframes(self):\n",
        "        if self.use_loadfp:\n",
        "            os.chdir(\"generated-reports\") # this directory better exist lol\n",
        "            datestring = \"-\" + str(datetime.datetime.now().strftime(\"%d.%m.%Y-%H.%M.%S\"))\n",
        "            os.mkdir(datestring[1:])\n",
        "            os.chdir(datestring[1:])\n",
        "        else:\n",
        "            datestring = \"\"\n",
        "\n",
        "        self.historyDF = pd.DataFrame(self.history)\n",
        "        self.historyDF.sort_values(by=['Time'], inplace=True)\n",
        "        self.historyDF.to_csv(\"report-history.csv\", index=False)\n",
        "\n",
        "        self.artistsDF = pd.DataFrame(self.artists)\n",
        "        self.artistsDF.sort_values(by=['Artist'], inplace=True)\n",
        "        self.artistsDF.to_csv(\"report-artists.csv\", index=False)\n",
        "\n",
        "        self.songsDF = pd.DataFrame(self.history)\n",
        "        self.total_songs = len(self.songsDF)\n",
        "        # here is where a better duplicate-dropping method would go to detect same songs\n",
        "        self.songsDF.drop_duplicates(subset=['URL'], inplace=True)\n",
        "        self.unique_songs = len(self.songsDF)\n",
        "        self.songsDF = self.songsDF.reset_index(drop=True)\n",
        "        self.songsDF.drop(columns=['Time'])\n",
        "        self.songsDF.to_csv(\"report-songs.csv\", index=False)\n",
        "\n",
        "        if self.use_loadfp:\n",
        "            self.loadfp = os.getcwd()\n",
        "            os.chdir(\"../../\")\n",
        "        else:\n",
        "            self.loadfp = os.getcwd() + \"/generated-reports/\" + datestring\n",
        "\n",
        "    # API management functions\n",
        "    @staticmethod\n",
        "    def parse_duration(duration):\n",
        "        # lol i hate this garbage but its not my code --> not my problem\n",
        "        timestr = duration\n",
        "        time = re.findall(r'\\d+', timestr)\n",
        "        length = len(time)\n",
        "        if length > 4:\n",
        "            return 0\n",
        "        if length == 4:\n",
        "            return ((int(time[0])*24*60*60)+(int(time[1])*60*60)+int(time[2]*60)+(int(time[3])))\n",
        "        elif length == 3:\n",
        "            return ((int(time[0])*60*60)+(int(time[1])*60)+(int(time[2])))\n",
        "        elif length == 2:\n",
        "            return ((int(time[0])*60)+(int(time[1])))\n",
        "        elif length == 1:\n",
        "            return (int(time[0]))\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def call_api(self, idlist):\n",
        "        parameters = {\"part\": \"contentDetails,snippet\", \"id\": ','.join(idlist), \"key\": self.apikey}\n",
        "        response = requests.get(\"https://www.googleapis.com/youtube/v3/videos\", params=parameters)\n",
        "\n",
        "        if (response.status_code == 200):\n",
        "            json_parsed = response.json()\n",
        "            for item in json_parsed['items']:\n",
        "                duration = Loader.parse_duration(item['contentDetails']['duration'])\n",
        "                url = item['id']\n",
        "                # for whatever reason this is necessary (assumes low value durations are stored in mins (which they ARE?! usually))\n",
        "                if duration < 10:\n",
        "                    duration = duration * 60\n",
        "\n",
        "                # update by url\n",
        "                for (j, i) in enumerate(self.history[\"URL\"]):\n",
        "                    if i == url:\n",
        "                        if duration >= 10:\n",
        "                            self.history[\"Duration\"][j] = duration\n",
        "        else:\n",
        "            print(\"Failed API call at\", idlist)\n",
        "\n",
        "    @staticmethod\n",
        "    def progress(value, max=100):\n",
        "        return HTML(\"\"\"\n",
        "            <progress\n",
        "                value='{value}'\n",
        "                max='{max}',\n",
        "                style='width: 100%'\n",
        "            >\n",
        "                {value}\n",
        "            </progress>\n",
        "        \"\"\".format(value=value, max=max))\n",
        "\n",
        "    def gen_durations(self):\n",
        "        # Count duration\n",
        "        idlist = []\n",
        "        calls = 0\n",
        "        unique_song_urls = set(self.history['URL'])\n",
        "        len_usurl = len(unique_song_urls)\n",
        "        print(\"Getting durations. This may take a while. Awaiting\", len_usurl, \"requests.\")\n",
        "        for url in unique_song_urls:\n",
        "            idlist.append(url)\n",
        "            if len(idlist) == 50:\n",
        "                self.out.update(Loader.progress(((1+50*calls)*100)/len_usurl, 100))\n",
        "                self.call_api(idlist)\n",
        "                calls += 1\n",
        "                idlist = []\n",
        "        self.out.update(Loader.progress(100, 100))\n",
        "        self.call_api(idlist)\n",
        "\n",
        "        # update artist durations\n",
        "        artist_durations = defaultdict(int)\n",
        "        for i in range(len(self.history[\"Artist\"])):\n",
        "            artist = self.history[\"Artist\"][i]\n",
        "            duration = self.history[\"Duration\"][i]\n",
        "            artist_durations[artist] += duration\n",
        "\n",
        "        occurrences = collections.Counter(self.history[\"Artist\"])\n",
        "        artists_dict = collections.defaultdict(list)\n",
        "        for i in (artist_durations, occurrences):\n",
        "            for key, val in i.items():\n",
        "                artists_dict[key].append(val)\n",
        "\n",
        "        durations = []\n",
        "        occurrences = []\n",
        "        for i, j in artists_dict.values():\n",
        "            durations.append(i)\n",
        "            occurrences.append(j)\n",
        "\n",
        "        self.artists = {\"Artist\": artists_dict.keys(), \"Occurrences\": occurrences, \"Duration\": durations}\n",
        "\n",
        "    def gen_blank_artists(self):\n",
        "        occurrences = dict(collections.Counter(self.history[\"Artist\"]))\n",
        "        artists_list = []\n",
        "        occurrences_list = []\n",
        "        for i, j in occurrences.items():\n",
        "            artists_list.append(i)\n",
        "            occurrences_list.append(j)\n",
        "\n",
        "        self.artists = {\"Artist\": artists_list, \"Occurrences\": occurrences_list}\n",
        "\n",
        "    def load(self):\n",
        "        print(\"Loading your preprocessed history files\")\n",
        "        self.historyDF = pd.read_csv(self.open_file(os.path.join(self.loadfp, \"report-history.csv\")), index_col=0)\n",
        "        self.artistsDF = pd.read_csv(self.open_file(os.path.join(self.loadfp, \"report-artists.csv\")), index_col=0)\n",
        "        self.songsDF = pd.read_csv(self.open_file(os.path.join(self.loadfp, \"report-songs.csv\")), index_col=0)\n",
        "        self.historyDF['Time'] = pd.to_datetime(self.historyDF['Time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "        return self.historyDF, self.artistsDF, self.songsDF\n",
        "\n",
        "    def outs(self):\n",
        "        if self.load_reports:\n",
        "            self.historyDF, self.artistsDF, self.songsDF = self.load()\n",
        "        else:\n",
        "            print(\"Processing history file\")\n",
        "            self.parse_json()\n",
        "            if self.use_duration:\n",
        "                self.gen_durations()\n",
        "            else:\n",
        "                self.gen_blank_artists()\n",
        "            self.gen_dataframes() # generates dataframes and writes to CSV\n",
        "            self.historyDF, self.artistsDF, self.songsDF = self.load()\n",
        "        print(\"Done!\")\n",
        "        return self.historyDF, self.artistsDF, self.songsDF"
      ],
      "metadata": {
        "id": "_MN-ZLgfjMBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzer class"
      ],
      "metadata": {
        "id": "FmltjvtB8aHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyqw36-9So1d"
      },
      "outputs": [],
      "source": [
        "class Analyzer():\n",
        "    def __init__(self, historyDF, artistsDF, songsDF, use_duration=True):\n",
        "        self.history = historyDF.reset_index()\n",
        "        self.artists = artistsDF\n",
        "        self.songs = songsDF\n",
        "        self.use_duration = use_duration\n",
        "\n",
        "        self.history['Date'] = self.history['Time'].apply(lambda x : x.year * 10000 + x.month * 100 + x.day)\n",
        "        self.first_last_dates = (min(self.history['Time']), max(self.history['Time']))\n",
        "\n",
        "    def tops(self, n=10):\n",
        "        # Top 10 Songs\n",
        "        tops = {}\n",
        "        tops['Top ' + str(n) + ' songs by count'] = self.songs.nlargest(n, ['Occurrences'])\n",
        "        tops['Top ' + str(n) + ' artists by count'] = self.artists.nlargest(n, ['Occurrences'])\n",
        "        tops['Top ' + str(n) + ' artists by time listened'] = self.artists.nlargest(n, ['Duration'])\n",
        "        return tops\n",
        "\n",
        "    def basic(self):\n",
        "        basic = {}\n",
        "        basic[\"Total seconds\"] = sum(self.history[\"Duration\"])\n",
        "        basic[\"Total songs\"] = len(self.history[\"Title\"])\n",
        "        basic[\"Unique songs\"] = len(self.songs[\"Title\"])\n",
        "        basic[\"Unique artists\"] = len(self.artists[\"Artist\"])\n",
        "        return basic\n",
        "\n",
        "    def uniques(self, n=10):\n",
        "        uniques = {}\n",
        "        uniques['Top ' + str(n) + ' artists by count of unique songs in history'] = collections.Counter(self.songs[\"Artist\"]).most_common(n)\n",
        "        return uniques\n",
        "\n",
        "    def repeats(self, n=10):\n",
        "        repeats = {}\n",
        "        grouped_history = [(_, len(list(i))) for _,i in itertools.groupby(self.history[\"URL\"])]\n",
        "        grouped_history.sort(key = lambda x : x[1], reverse=True)\n",
        "        grouped_songs = self.songs[self.songs['URL'].isin([i[0] for i in grouped_history[:n]])]\n",
        "        repeats[str(n) + ' most consecutively repeated songs'] = grouped_songs\n",
        "\n",
        "        return repeats\n",
        "\n",
        "    def chronology(self, n1=5, n2=5, tz=\"US/Eastern\"):\n",
        "        chronology = {}\n",
        "\n",
        "        idx = 0\n",
        "        top_songs_per_month = {}\n",
        "        year = self.first_last_dates[0].year\n",
        "        for month in range((self.first_last_dates[1].month + self.first_last_dates[1].year * 12) - (self.first_last_dates[0].month + self.first_last_dates[0].year * 12) + 1):\n",
        "            if (month + 1) % 12 == 1 and month != 0:\n",
        "                year += 1\n",
        "            month %= 12\n",
        "            month += 1\n",
        "            nidx = idx\n",
        "            while nidx < len(self.history['Time']) and self.history['Time'][nidx].month == month and self.history['Time'][nidx].year == year:\n",
        "                nidx += 1\n",
        "            songs_for_month = self.history[idx:nidx]\n",
        "            songs_for_month = songs_for_month.sort_values(by='URL', key=lambda x: x.map(songs_for_month['URL'].value_counts()), ascending=False)\n",
        "            songs_for_month['Occurrences'] = songs_for_month.groupby('URL')['URL'].transform('size')\n",
        "            songs_for_month = songs_for_month.drop_duplicates(subset=['URL'])[:n1]\n",
        "            idx = nidx\n",
        "            top_songs_per_month[datetime.datetime(year=year, month=month, day=1).strftime(\"%b %Y\")] = songs_for_month\n",
        "\n",
        "        chronology[\"Top \" + str(n1) + \" songs per month\"] = top_songs_per_month\n",
        "\n",
        "        by_date = self.history.groupby(by=['Date'], as_index=False).sum(['Duration'])\n",
        "        by_date['Date'] = by_date['Date'].apply(lambda x : datetime.datetime(year=int(x/10000), month=int(x%10000/100), day=int(x%10000%100)))\n",
        "        top_by_date = by_date.sort_values(by=['Duration'], ascending=False)[:n2]\n",
        "        chronology[\"Top \" + str(n2) + \" musical days\"] = top_by_date\n",
        "        chronology[\"Duration by date\"] = by_date\n",
        "\n",
        "        localtime = pytz.timezone(tz)\n",
        "        now = datetime.datetime.now()\n",
        "        chronology[\"Songs per time of day\"] = defaultdict(int)\n",
        "        chronology[\"Songs per day of week\"] = defaultdict(int)\n",
        "        chronology[\"Songs per month of year\"] = defaultdict(int)\n",
        "        for i in self.history[\"Time\"]:\n",
        "            x = i.hour\n",
        "            if bool(localtime.localize(i).dst()):\n",
        "                x += 1\n",
        "            x -= 24 - (localtime.utcoffset(now).seconds / 3600)\n",
        "            chronology[\"Songs per time of day\"][x%24] += 1\n",
        "            chronology[\"Songs per day of week\"][i.weekday()] += 1\n",
        "            chronology[\"Songs per month of year\"][i.month] += 1\n",
        "\n",
        "        return chronology\n",
        "\n",
        "    def averages(self):\n",
        "        averages = {}\n",
        "        averages[\"Weighted average song length\"] = sum(self.history[\"Duration\"]) / len(self.history[\"Duration\"])\n",
        "        averages[\"Average song length\"] = sum(self.songs[\"Duration\"]) / len(self.songs[\"Duration\"])\n",
        "        averages[\"Average seconds per day\"] = sum(self.history[\"Duration\"]) / len(set(self.history['Date']))\n",
        "        averages[\"Shortest song\"] = self.history.iloc[self.history['Duration'].idxmin()]\n",
        "        averages[\"Longest song\"] = self.history.iloc[self.history['Duration'].idxmax()]\n",
        "        averages[\"Median song length\"] = sorted(self.songs['Duration'])[len(self.songs)//2]\n",
        "        averages[\"Weighted median song length\"] = sorted(self.history['Duration'])[len(self.history)//2]\n",
        "        averages[\"Average replays per song\"] = sum(self.songs[\"Occurrences\"]) / len(self.songs[\"Occurrences\"])\n",
        "        averages[\"Max replays\"] = max(self.songs[\"Occurrences\"])\n",
        "        averages[\"Frequency of replays\"] = collections.Counter(self.songs[\"Occurrences\"])\n",
        "\n",
        "        return averages\n",
        "\n",
        "    def sessions(self):\n",
        "        # split history into sessions (need to decide what distinguishes 1 session from another, most likely 30mins)\n",
        "        # get longest session as a statistic\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution code"
      ],
      "metadata": {
        "id": "WgMjJSI48dMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/My Drive/Colab Notebooks/ytmwrapped\")"
      ],
      "metadata": {
        "id": "f_G-o8b3Q839"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loader.merge_jsons(\"watch-history-new.json\", \"watch-history.json\", \"watch-history-merged.json\")"
      ],
      "metadata": {
        "id": "uul91mtOJiO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loader code\n",
        "apikey = open(\"apitoken.txt\", \"r\").read()\n",
        "filepath = \"\" # watch-history-merged.json\n",
        "loadfp = os.getcwd()+\"/generated-reports/26.12.2023-15.16.42\"\n",
        "\n",
        "ignore = {}\n",
        "ignore[\"Title\"] = open(\"ignore-title.txt\", \"r\").read().split(\",\")\n",
        "ignore[\"Artist\"] = open(\"ignore-artist.txt\", \"r\").read().split(\",\")\n",
        "ignore[\"URL\"] = open(\"ignore-url.txt\", \"r\").read().split(\",\")\n",
        "\n",
        "loader = Loader(use_duration=True, analyze_years=[\"2021\", \"2022\", \"2023\"], apikey=apikey, filepath=filepath, loadfp=loadfp, ignores=ignore)\n",
        "history, artists, songs = loader.outs()"
      ],
      "metadata": {
        "id": "HffdIbELSzka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = Analyzer(history, artists, songs)\n",
        "basic, tops, uniques, repeats, chrono, averages = analyzer.basic(), analyzer.tops(), analyzer.uniques(), analyzer.repeats(), analyzer.chronology(), analyzer.averages()\n",
        "print(\"BASIC\")\n",
        "for i, j in basic.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"You spent \" + str(round(basic[\"Total seconds\"]/(365*24*60*60)/len(loader.analyze_years)*100, 2)) + \"% of the year (on average) listening to music!\")\n",
        "print(\"TOPS\")\n",
        "for i, j in tops.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"UNIQUES\")\n",
        "for i, j in uniques.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"REPEATS\")\n",
        "for i, j in repeats.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"CHRONOLOGY\")\n",
        "for i, j in chrono.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"AVERAGES\")\n",
        "for i, j in averages.items():\n",
        "    print(i)\n",
        "    print(j)"
      ],
      "metadata": {
        "id": "7Tm63id7JECV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "times = chrono[\"Songs per time of day\"]\n",
        "x = sorted(times.keys())\n",
        "y = []\n",
        "for i in x:\n",
        "    y.append(times[i])\n",
        "\n",
        "x = list(reversed(x))\n",
        "y = list(reversed(y))\n",
        "\n",
        "def rightRotate(lists, num):\n",
        "    output_list = []\n",
        "\n",
        "    for item in range(len(lists) - num, len(lists)):\n",
        "        output_list.append(lists[item])\n",
        "\n",
        "    for item in range(0, len(lists) - num):\n",
        "        output_list.append(lists[item])\n",
        "\n",
        "    return output_list\n",
        "\n",
        "x = rightRotate(x, 6)\n",
        "y = rightRotate(y, 6)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "plt.axis('off')\n",
        "\n",
        "upperLimit = max(y)\n",
        "lowerLimit = min(y)\n",
        "\n",
        "maxval = max(y)\n",
        "\n",
        "slope = (maxval - lowerLimit) / maxval\n",
        "heights = [slope * i + lowerLimit for i in y]\n",
        "\n",
        "width = 2*np.pi / 24\n",
        "\n",
        "indexes = list(range(1, 25))\n",
        "angles = [element * width for element in indexes]\n",
        "\n",
        "grey_heights = [slope*maxval + lowerLimit] * 24\n",
        "\n",
        "# Draw bars\n",
        "bars = ax.bar(\n",
        "    x=angles,\n",
        "    height=grey_heights,\n",
        "    width=width,\n",
        "    bottom=lowerLimit,\n",
        "    linewidth=2,\n",
        "    edgecolor=\"white\",\n",
        "    color=\"#d3d3d3\",\n",
        ")\n",
        "\n",
        "bars = ax.bar(\n",
        "    x=angles,\n",
        "    height=heights,\n",
        "    width=width,\n",
        "    bottom=lowerLimit,\n",
        "    linewidth=2,\n",
        "    edgecolor=\"white\",\n",
        "    color=\"#61a4b2\",\n",
        ")\n",
        "\n",
        "labelPadding = 5\n",
        "\n",
        "for bar, angle, height, label in zip(bars,angles, heights, [str(i) for i in x]):\n",
        "\n",
        "    rotation = np.rad2deg(angle)\n",
        "\n",
        "    alignment = \"\"\n",
        "    if angle >= np.pi/2 and angle < 3*np.pi/2:\n",
        "        alignment = \"right\"\n",
        "        rotation = rotation + 180\n",
        "    else:\n",
        "        alignment = \"left\"\n",
        "\n",
        "    if (height > 100):\n",
        "        ax.text(\n",
        "            x=angle,\n",
        "            y=lowerLimit + height + labelPadding,\n",
        "            s=label,\n",
        "            ha=alignment,\n",
        "            va='center',\n",
        "            rotation=rotation,\n",
        "            rotation_mode=\"anchor\")"
      ],
      "metadata": {
        "id": "Ad--FGUdjKg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekdays = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
        "plt.bar(weekdays, chrono[\"Songs per day of week\"].values())"
      ],
      "metadata": {
        "id": "r69b6bzJiWZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(chrono[\"Songs per month of year\"].keys(), chrono[\"Songs per month of year\"].values())"
      ],
      "metadata": {
        "id": "4znkRaF-74qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_buckets = 60\n",
        "plt.hist(list(analyzer.songs[\"Duration\"]), num_buckets, (0, 600))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qu6fogbtReGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(averages['Frequency of replays'].keys(), averages['Frequency of replays'].values())\n",
        "plt.yscale(\"linear\") # \"log\" or \"linear\""
      ],
      "metadata": {
        "id": "dCn-UcuK73_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MusicBrainz API"
      ],
      "metadata": {
        "id": "yxkOzBYWZxea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genres"
      ],
      "metadata": {
        "id": "tRGcRBc28Zpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allowed_vals_f = open('genres.txt', 'r')\n",
        "allowed_vals = []\n",
        "for i in allowed_vals_f:\n",
        "    allowed_vals.append(i.replace(\"\\n\", \"\"))"
      ],
      "metadata": {
        "id": "_dUJP3gJi_1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_tags = {}\n",
        "print(len(artists['Artist']), \"Artists\")\n",
        "for c, i in enumerate(artists['Artist']):\n",
        "    artist_tags[i] = []\n",
        "    response = requests.get(\"https://musicbrainz.org/ws/2/artist/?fmt=json&query=name:\" + urllib.parse.quote(i.strip()))\n",
        "    if c % 20 == 0:\n",
        "        print(\"Got\", c, \"artists\")\n",
        "    try:\n",
        "        tags = response.json()['artists'][0]['tags']\n",
        "        #print(tags)\n",
        "        for ii in tags:\n",
        "            if ii['name'] in allowed_vals:\n",
        "                artist_tags[i].append(ii['name'])\n",
        "    except:\n",
        "        #print(\"No tags for artist: \" + i)\n",
        "        pass"
      ],
      "metadata": {
        "id": "P3ahcMQZZzfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alltags = itertools.chain(*artist_tags.values())\n",
        "counter = collections.Counter(alltags).most_common(10)\n",
        "genres = []\n",
        "counts = []\n",
        "for (i, j) in counter:\n",
        "    genres.append(i)\n",
        "    counts.append(j)\n",
        "plt.pie(counts, labels=genres, autopct=\"%1.1f%%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XcrJBOmFm4JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alltags = []\n",
        "artist_tags = tags2\n",
        "for i in history['Artist']:\n",
        "    if isinstance(artist_tags[i], str):\n",
        "        alltags.append(artist_tags[i])\n",
        "    else:\n",
        "        for ii in artist_tags[i]:\n",
        "            alltags.append(ii)\n",
        "counter = collections.Counter(alltags).most_common(10)\n",
        "genres = []\n",
        "counts = []\n",
        "for (i, j) in counter:\n",
        "    genres.append(i)\n",
        "    counts.append(j)\n",
        "plt.pie(counts, labels=genres, autopct=\"%1.1f%%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWMMNMxU9C02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in tags2.items():\n",
        "    if j == []:\n",
        "        print(i)\n",
        "        tags2[i] = input()"
      ],
      "metadata": {
        "id": "JHTepiXk-05T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('artist_tags.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(artist_tags))"
      ],
      "metadata": {
        "id": "tacFdinnqpQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('artist_tags.txt', 'r') as convert_file:\n",
        "     artist_tags = ast.literal_eval(convert_file.read())"
      ],
      "metadata": {
        "id": "GzvTIwcp3enG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Year"
      ],
      "metadata": {
        "id": "FTwbhcCw8gj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just getting genre by genre of artist, not release (because release definitely has some issues)\n",
        "arid = {}\n",
        "ardate = {}\n",
        "count = {}\n",
        "for c, i in enumerate(artists['Artist']):\n",
        "    i = i.strip()\n",
        "    if c%40 == 0:\n",
        "        print(\"Got\", c, \"artists\")\n",
        "    try:\n",
        "        response = requests.get(\"https://musicbrainz.org/ws/2/artist/?fmt=json&query=name:\" + urllib.parse.quote(i))\n",
        "        arid[i] = response.json()['artists'][0]['id']\n",
        "        # store the date in the thing\n",
        "        response = requests.get(\"https://musicbrainz.org/ws/2/release-group/?fmt=json&query=arid:\" + arid[i])\n",
        "        frd = int(response.json()['release-groups'][0]['first-release-date'][0:4])\n",
        "        for ii in response.json()['release-groups']:\n",
        "            try:\n",
        "                t = ii['first-release-date']\n",
        "                if int(t[0:4]) < frd:\n",
        "                    frd = int(t[0:4])\n",
        "            except:\n",
        "                continue\n",
        "        ardate[i] = frd\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "IslM3ZPG8iQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = {}\n",
        "for i in history['Artist']:\n",
        "    i = i.strip()\n",
        "    if i not in ardate.keys():\n",
        "        continue\n",
        "    if ardate[i] in count.keys():\n",
        "        count[ardate[i]] += 1\n",
        "    else:\n",
        "        count[ardate[i]] = 1"
      ],
      "metadata": {
        "id": "vpGBeXMbJbfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(count.keys(), count.values())"
      ],
      "metadata": {
        "id": "-f15bXYzCzGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}