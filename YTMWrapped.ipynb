{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "iq7_lIIX8Xe2",
        "tRGcRBc28Zpi",
        "FTwbhcCw8gj5"
      ],
      "authorship_tag": "ABX9TyNGEx0rUUDWCnRVEIB5vp4f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DO NOT BLINDLY RUN ALL IN THIS NOTEBOOK!!**"
      ],
      "metadata": {
        "id": "k73igQ9dMh-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo\n",
        "ANALYSIS\n",
        "- fix `chronology()` and `averages()` in `Analyzer`\n",
        "- restore most active day (currently broken) in `Analyzer`\n",
        "- add `sessions()` in `Analyzer` for longest listening session\n",
        "\n",
        "NEW FEATURES\n",
        "- matplotlib & GUI libraries for data vis (until more aesthetic things can be found) (**WIP**)\n",
        "- better skip detection via timestamps of listen & duration of song (in Loader)\n",
        "- option to store/load database when loading for API call optimization\n",
        "- add musicbrainz (or another) API to get genre and decade and album and other really good song data (**WIP**)\n",
        "- analyze user playlists (this will be a little tougher)\n",
        "- bring back proper pretty generated reports\n",
        "\n",
        "GENERAL USABILITY\n",
        "- make `use_duration` mandatory, remove relevant flags\n",
        "- store `time` as datetime and not string\n",
        "- fix `merge_jsons()` so that it works in all cases\n",
        "- fix functions to account for\n",
        "  - days when music wasn't listened to\n",
        "  - leap years\n",
        "  - duplicates (e.g. same songs but different URLs)\n",
        "  - timezones\n",
        "- move imports to necessary sections\n",
        "- add basic info and `input()`s to help any user generate their report\n",
        "\n",
        "LONG TERM\n",
        "- understand songs' relation to one another by their sequence (start doing ML using song metadata, essentially)"
      ],
      "metadata": {
        "id": "RXRnkI-DXCJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for the ENTIRE program (not all are necessary for each individual section)\n",
        "import datetime\n",
        "import sys\n",
        "import getopt\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import collections\n",
        "import os\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import pytz\n",
        "import itertools\n",
        "import numpy as np\n",
        "import urllib.parse\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "import ast\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "ge083-6rStL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# force remount if drive connection is broken/not syncing\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ptEQU3aVJZFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loader class"
      ],
      "metadata": {
        "id": "iq7_lIIX8Xe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loader():\n",
        "    def __init__(self, use_duration, analyze_years, apikey, filepath, loadfp, ignores={\"Title\": [], \"Artist\": [], \"URL\": []}):\n",
        "        self.use_duration, self.analyze_years, self.apikey, self.filepath, self.loadfp, self.ignores = use_duration, analyze_years, apikey, filepath, loadfp, ignores\n",
        "        self.load_reports = filepath != \"\"\n",
        "        self.use_loadfp = loadfp != \"\"\n",
        "        if self.load:\n",
        "            self.file = self.open_file(self.filepath)\n",
        "        else:\n",
        "            self.file = None\n",
        "        self.out = display(Loader.progress(0, 100), display_id=True)\n",
        "\n",
        "    # utility methods\n",
        "    def should_not_ignore(self, obj):\n",
        "        if 'subtitles' in obj and obj['header'] == \"YouTube Music\" and obj['title'][:7] == \"Watched\" and obj['time'][:4] in self.analyze_years:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def open_file(filepath):\n",
        "        try:\n",
        "            file = open(filepath, \"r\", encoding=\"utf8\")\n",
        "            return file\n",
        "        except:\n",
        "            print(\"There was an error opening your report files\")\n",
        "\n",
        "    # takes two watch history files and merges them together (useful for takeout files which have limited date range)\n",
        "    # only works if original jsons are already sorted by date and the first argument is chronologically ahead of the second argument and the two jsons overlap\n",
        "    @staticmethod\n",
        "    def merge_jsons(file1path, file2path, outputfilepath):\n",
        "        print(\"WARN: merge_jsons() was called. Be sure that its criteria are met before using the output of this method!\")\n",
        "        file2 = Loader.open_file(file2path)\n",
        "        json2 = json.load(file2)\n",
        "        date, datestr = \"\", \"\"\n",
        "        for obj in json2:\n",
        "            if 'subtitles' in obj:\n",
        "                datestr = obj['time'][:-5]\n",
        "                date = datetime.datetime.strptime(datestr, \"%Y-%m-%dT%H:%M:%S\")\n",
        "                break\n",
        "\n",
        "        combined = \"\"\n",
        "        file1 = Loader.open_file(file1path)\n",
        "        file2 = Loader.open_file(file2path)\n",
        "        for i in file1.readlines():\n",
        "            combined += i\n",
        "            if datestr in i:\n",
        "                combined+=\"\\\"merged\\\":\\\"here\\\"},\"\n",
        "                break\n",
        "\n",
        "        for c, i in enumerate(file2.readlines()):\n",
        "            if c == 0:\n",
        "                i = i[1:]\n",
        "            combined += i\n",
        "\n",
        "        new_file = open(outputfilepath, \"wt\")\n",
        "        new_file.write(combined)\n",
        "        new_file.close()\n",
        "\n",
        "    # processes bulk lists\n",
        "    def parse_json(self):\n",
        "        self.history = {\"Title\": [], \"Artist\": [], \"Year\": [], \"URL\": [], \"Duration\": []}\n",
        "        json_object = json.load(self.file)\n",
        "        for obj in json_object:\n",
        "            if (self.should_not_ignore(obj)):\n",
        "                self.history[\"Title\"].append(obj['title'][8:])\n",
        "                self.history[\"Artist\"].append(obj['subtitles'][0]['name'].replace('- Topic ', '').replace('- Topic', ''))\n",
        "                self.history[\"Year\"].append(obj['time'])\n",
        "                self.history[\"URL\"].append(obj['titleUrl'][obj['titleUrl'].index('v=')+2:])\n",
        "                self.history[\"Duration\"].append(0)\n",
        "\n",
        "        # preprocessing\n",
        "        for i in range(len(self.history[\"Title\"])):\n",
        "            if i >= len(self.history[\"Title\"]):\n",
        "                break # contingency\n",
        "            if self.history[\"Title\"][i] in self.ignores[\"Title\"] or self.history[\"Artist\"][i] in self.ignores[\"Artist\"] or self.history[\"URL\"][i] in self.ignores[\"URL\"]:\n",
        "                del self.history[\"Title\"][i]\n",
        "                del self.history[\"Artist\"][i]\n",
        "                del self.history[\"Year\"][i]\n",
        "                del self.history[\"URL\"][i]\n",
        "                del self.history[\"Duration\"][i]\n",
        "                i-= 1\n",
        "\n",
        "        occurrences = collections.Counter(self.history['URL'])\n",
        "        self.history['Occurrences'] = []\n",
        "        for i in self.history['URL']:\n",
        "            self.history['Occurrences'].append(occurrences[i])\n",
        "\n",
        "        occurrences = collections.Counter(self.history['Artist'])\n",
        "        duration = [0]*len(occurrences.keys())\n",
        "        self.artists = {\"Artist\": occurrences.keys(), \"Occurrences\": occurrences.values(), \"Duration\": duration}\n",
        "\n",
        "    # generates dataframes and csv files\n",
        "    def gen_dataframes(self):\n",
        "        if self.use_loadfp:\n",
        "            os.chdir(\"generated-reports\") # this directory better exist lol\n",
        "            datestring = \"-\" + str(datetime.datetime.now().strftime(\"%d.%m.%Y-%H.%M.%S\"))\n",
        "            os.mkdir(datestring[1:])\n",
        "            os.chdir(datestring[1:])\n",
        "        else:\n",
        "            datestring = \"\"\n",
        "\n",
        "        self.historyDF = pd.DataFrame(self.history)\n",
        "        self.historyDF.to_csv(\"report-history.csv\")\n",
        "\n",
        "        self.artistsDF = pd.DataFrame(self.artists)\n",
        "        self.artistsDF.to_csv(\"report-artists.csv\")\n",
        "\n",
        "        self.songsDF = pd.DataFrame(self.history)\n",
        "        self.total_songs = len(self.songsDF)\n",
        "        # here is where a better duplicate-dropping method would go to detect same songs\n",
        "        self.songsDF.drop_duplicates(subset=['URL'], inplace=True)\n",
        "        self.unique_songs = len(self.songsDF)\n",
        "        self.songsDF = self.songsDF.reset_index(drop=True)\n",
        "        self.songsDF.to_csv(\"report-songs.csv\")\n",
        "\n",
        "        if self.use_loadfp:\n",
        "            os.chdir(\"../../\")\n",
        "\n",
        "    # API management functions\n",
        "    @staticmethod\n",
        "    def parse_duration(duration):\n",
        "        # lol i hate this garbage but its not my code --> not my problem\n",
        "        timestr = duration\n",
        "        time = re.findall(r'\\d+', timestr)\n",
        "        length = len(time)\n",
        "        if length > 4:\n",
        "            return 0\n",
        "        if length == 4:\n",
        "            return ((int(time[0])*24*60*60)+(int(time[1])*60*60)+int(time[2]*60)+(int(time[3])))\n",
        "        elif length == 3:\n",
        "            return ((int(time[0])*60*60)+(int(time[1])*60)+(int(time[2])))\n",
        "        elif length == 2:\n",
        "            return ((int(time[0])*60)+(int(time[1])))\n",
        "        elif length == 1:\n",
        "            return (int(time[0]))\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def call_api(self, idlist):\n",
        "        parameters = {\"part\": \"contentDetails,snippet\", \"id\": ','.join(idlist), \"key\": self.apikey}\n",
        "        response = requests.get(\"https://www.googleapis.com/youtube/v3/videos\", params=parameters)\n",
        "\n",
        "        if (response.status_code == 200):\n",
        "            json_parsed = response.json()\n",
        "            for item in json_parsed['items']:\n",
        "                duration = Loader.parse_duration(item['contentDetails']['duration'])\n",
        "                url = item['id']\n",
        "                # for whatever reason this is necessary (assumes low value durations are stored in mins (which they ARE?! usually))\n",
        "                if duration < 10:\n",
        "                    duration = duration * 60\n",
        "\n",
        "                # update by url\n",
        "                for (j, i) in enumerate(self.history[\"URL\"]):\n",
        "                    if i == url:\n",
        "                        if duration >= 10:\n",
        "                            self.history[\"Duration\"][j] = duration\n",
        "        else:\n",
        "            print(\"Failed API call at\", idlist)\n",
        "\n",
        "    @staticmethod\n",
        "    def progress(value, max=100):\n",
        "        return HTML(\"\"\"\n",
        "            <progress\n",
        "                value='{value}'\n",
        "                max='{max}',\n",
        "                style='width: 100%'\n",
        "            >\n",
        "                {value}\n",
        "            </progress>\n",
        "        \"\"\".format(value=value, max=max))\n",
        "\n",
        "    def gen_durations(self):\n",
        "        # Count duration\n",
        "        idlist = []\n",
        "        calls = 0\n",
        "        unique_song_urls = set(self.history['URL'])\n",
        "        len_usurl = len(unique_song_urls)\n",
        "        print(\"Getting durations. This may take a while. Awaiting\", len_usurl, \"requests.\")\n",
        "        for url in unique_song_urls:\n",
        "            idlist.append(url)\n",
        "            if len(idlist) == 50:\n",
        "                self.out.update(Loader.progress(((1+50*calls)*100)/len_usurl, 100))\n",
        "                self.call_api(idlist)\n",
        "                calls += 1\n",
        "                idlist = []\n",
        "        self.out.update(Loader.progress(100, 100))\n",
        "        self.call_api(idlist)\n",
        "\n",
        "        # update artist durations\n",
        "        artist_durations = defaultdict(int)\n",
        "        for i in range(len(self.history[\"Artist\"])):\n",
        "            artist = self.history[\"Artist\"][i]\n",
        "            duration = self.history[\"Duration\"][i]\n",
        "            artist_durations[artist] += duration\n",
        "\n",
        "        occurrences = collections.Counter(self.history[\"Artist\"])\n",
        "        artists_dict = collections.defaultdict(list)\n",
        "        for i in (artist_durations, occurrences):\n",
        "            for key, val in i.items():\n",
        "                artists_dict[key].append(val)\n",
        "\n",
        "        durations = []\n",
        "        occurrences = []\n",
        "        for i, j in artists_dict.values():\n",
        "            durations.append(i)\n",
        "            occurrences.append(j)\n",
        "\n",
        "        self.artists = {\"Artist\": artists_dict.keys(), \"Occurrences\": occurrences, \"Duration\": durations}\n",
        "\n",
        "        self.gen_dataframes()\n",
        "\n",
        "    def gen_blank_artists(self):\n",
        "        occurrences = dict(collections.Counter(self.history[\"Artist\"]))\n",
        "        artists_list = []\n",
        "        occurrences_list = []\n",
        "        for i, j in occurrences.items():\n",
        "            artists_list.append(i)\n",
        "            occurrences_list.append(j)\n",
        "\n",
        "        self.artists = {\"Artist\": artists_list, \"Occurrences\": occurrences_list}\n",
        "\n",
        "    def outs(self):\n",
        "        if self.load_reports:\n",
        "            self.historyDF, self.artistsDF, self.songsDF = self.load()\n",
        "        else:\n",
        "            print(\"We are now processing your file\")\n",
        "            self.parse_json()\n",
        "            if self.use_duration:\n",
        "                self.gen_durations()\n",
        "            else:\n",
        "                self.gen_blank_artists()\n",
        "            self.gen_dataframes() # generates dataframes and writes to CSV\n",
        "\n",
        "        return self.historyDF, self.artistsDF, self.songsDF\n",
        "\n",
        "    def load(self):\n",
        "        print(\"Loading your preprocessed history files\")\n",
        "        historyDF = pd.read_csv(self.open_file(os.path.join(self.loadfp, \"report-history.csv\")))\n",
        "        artistsDF = pd.read_csv(self.open_file(os.path.join(self.loadfp, \"report-artists.csv\")))\n",
        "        songsDF = pd.read_csv(self.open_file(os.path.join(self.loadfp, \"report-songs.csv\")))\n",
        "        return historyDF, artistsDF, songsDF"
      ],
      "metadata": {
        "id": "_MN-ZLgfjMBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzer class"
      ],
      "metadata": {
        "id": "FmltjvtB8aHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyqw36-9So1d"
      },
      "outputs": [],
      "source": [
        "class Analyzer():\n",
        "    def __init__(self, historyDF, artistsDF, songsDF, use_duration=True):\n",
        "        self.history = historyDF\n",
        "        self.artists = artistsDF\n",
        "        self.songs = songsDF\n",
        "        self.use_duration = use_duration\n",
        "\n",
        "    def tops(self, n=10):\n",
        "        # Top 10 Songs\n",
        "        tops = {}\n",
        "        tops['Top ' + str(n) + ' songs by count'] = self.songs.nlargest(n, ['Occurrences'])\n",
        "        tops['Top ' + str(n) + ' artists by count'] = self.artists.nlargest(n, ['Occurrences'])\n",
        "        tops['Top ' + str(n) + ' artists by time listened'] = self.artists.nlargest(n, ['Duration'])\n",
        "        return tops\n",
        "\n",
        "    def basic(self):\n",
        "        basic = {}\n",
        "        basic[\"Total seconds\"] = sum(self.history[\"Duration\"])\n",
        "        basic[\"Total songs\"] = len(self.history[\"Title\"])\n",
        "        basic[\"Unique songs\"] = len(self.songs[\"Title\"])\n",
        "        basic[\"Unique artists\"] = len(self.artists[\"Artist\"])\n",
        "        return basic\n",
        "\n",
        "    def uniques(self, n=10):\n",
        "        uniques = {}\n",
        "        uniques['Top ' + str(n) + ' artists by count of unique songs in history'] = collections.Counter(self.songs[\"Artist\"]).most_common(n)\n",
        "        return uniques\n",
        "\n",
        "    def repeats(self, n=10):\n",
        "        repeats = {}\n",
        "        grouped_history = [(_, len(list(i))) for _,i in itertools.groupby(self.history[\"URL\"])]\n",
        "        grouped_history.sort(key = lambda x : x[1], reverse=True)\n",
        "        grouped_songs = self.songs[self.songs['URL'].isin([i[0] for i in grouped_history[:n]])]\n",
        "        repeats[str(n) + ' most consecutively repeated songs'] = grouped_songs\n",
        "\n",
        "        return repeats\n",
        "\n",
        "    # TODO FIX BELOW\n",
        "    def chronology(self, n=3):\n",
        "        # this stuff will break if you are analyzing a period longer than a year\n",
        "\n",
        "        chronology = {}\n",
        "        top_songs_per_month = []\n",
        "        for month in range(12):\n",
        "            songs_for_month = {\"Title\": [], \"Artist\": [], \"URL\": [], \"Duration\": []}\n",
        "            for j,i in enumerate(self.history[\"Year\"]):\n",
        "                if int(i[5:7]) == month+1:\n",
        "                    songs_for_month[\"Title\"].append(self.history[\"Title\"][j])\n",
        "                    songs_for_month[\"Artist\"].append(self.history[\"Artist\"][j])\n",
        "                    songs_for_month[\"URL\"].append(self.history[\"URL\"][j])\n",
        "                    songs_for_month[\"Duration\"].append(self.history[\"Duration\"][j])\n",
        "\n",
        "            occurrences = collections.Counter(songs_for_month['URL'])\n",
        "            songs_for_month['Occurrences'] = []\n",
        "            for i in songs_for_month['URL']:\n",
        "                songs_for_month['Occurrences'].append(occurrences[i])\n",
        "            songs_for_month_DF = pd.DataFrame(songs_for_month)\n",
        "            songs_for_month_DF.drop_duplicates(subset=['URL'], inplace=True)\n",
        "            top_songs_per_month.append(songs_for_month_DF.nlargest(n, ['Occurrences']))\n",
        "\n",
        "        chronology[\"Top n Songs Per Month\"] = top_songs_per_month\n",
        "\n",
        "        days = {}\n",
        "        for j, i in enumerate(self.history[\"Year\"]):\n",
        "            try:\n",
        "                days[i[5:10]][\"Title\"].append(self.history[\"Title\"][j])\n",
        "                days[i[5:10]][\"Artist\"].append(self.history[\"Artist\"][j])\n",
        "                days[i[5:10]][\"URL\"].append(self.history[\"URL\"][j])\n",
        "                days[i[5:10]][\"Duration\"].append(self.history[\"Duration\"][j])\n",
        "            except:\n",
        "                songs_for_day = {\"Title\": [self.history[\"Title\"][j]], \"Artist\": [self.history[\"Artist\"][j]], \"URL\": [self.history[\"URL\"][j]], \"Duration\": [self.history[\"Duration\"][j]]}\n",
        "                days[i[5:10]] = songs_for_day\n",
        "\n",
        "        \"\"\"\n",
        "        # TEMPORARILY BLOCKED BECAUSE ITS SUPER BROKEN\n",
        "\n",
        "        day_most_listened = -1 # day you listened to the most music function\n",
        "        durations_per_day = [] # durations per day function\n",
        "        songs_listened_dml = -1\n",
        "        for j, i in enumerate(days.values()):\n",
        "            urls = collections.Counter(i[\"URL\"])\n",
        "            if ((len(urls.values())) > songs_listened_dml):\n",
        "                day_most_listened = j\n",
        "                songs_listened_dml = len(urls.values())\n",
        "            durations_per_day.append(sum(i[\"Duration\"]))\n",
        "\n",
        "        # since it counts from NOW to the past, this is in reverse order (this breaks of day 1 of the dataset isn't jan 1... uh oh)\n",
        "        chronology[\"Most Diverse Day\"] = [len(days) - day_most_listened , durations_per_day[day_most_listened]//60, songs_listened_dml] # this is the Nth day of the year\n",
        "        chronology[\"Durations Per Day\"] = durations_per_day\n",
        "        chronology[\"Most Musical Day\"] = [len(days) - durations_per_day.index(max(durations_per_day)), max(durations_per_day)//60]\n",
        "        \"\"\"\n",
        "\n",
        "        times = {}\n",
        "        localtime = pytz.timezone(\"US/Eastern\")\n",
        "        hrs = [i for i in range(0, 24)]\n",
        "        errs = 0\n",
        "        chronology[\"Songs Per Time of Day\"] = dict()\n",
        "        for i in range(24):\n",
        "            chronology[\"Songs Per Time of Day\"][i] = 0\n",
        "        for i in self.history[\"Year\"]:\n",
        "            try:\n",
        "                # am I tripping or does this DST thing make no sense at all\n",
        "                isDST = bool(localtime.localize(datetime.datetime.strptime(i[:-5], \"%Y-%m-%dT%H:%M:%S\")).dst())\n",
        "                x = int(i[11:13])\n",
        "                if isDST:\n",
        "                    x += 1\n",
        "                x -= 5 # needs to better account for time zone!\n",
        "                x = hrs[x]\n",
        "                try:\n",
        "                    times[x] += 1\n",
        "                except:\n",
        "                    times[x] = 1\n",
        "            except:\n",
        "                errs += 1\n",
        "\n",
        "        for i, j in times.items():\n",
        "            chronology[\"Songs Per Time of Day\"][i] = j\n",
        "        chronology[\"Songs Per Time of Day Errors\"] = errs\n",
        "\n",
        "        weekdays = {}\n",
        "        errs = 0\n",
        "        for i in self.history[\"Year\"]:\n",
        "            try:\n",
        "                wd = datetime.datetime.strptime(i[:-5], \"%Y-%m-%dT%H:%M:%S\").weekday()\n",
        "                try:\n",
        "                    weekdays[wd] += 1\n",
        "                except:\n",
        "                    weekdays[wd] = 1\n",
        "            except:\n",
        "                errs += 1\n",
        "\n",
        "        chronology[\"Days of the Week\"] = weekdays\n",
        "        chronology[\"Days of the Week Errors\"] = errs\n",
        "\n",
        "        # this code can technically be inserted earlier when we process months\n",
        "        months = {}\n",
        "        for i in self.history[\"Year\"]:\n",
        "            try:\n",
        "                months[int(i[5:7])] += 1\n",
        "            except:\n",
        "                months[int(i[5:7])] = 1\n",
        "\n",
        "        chronology[\"Songs Per Month\"] = months\n",
        "\n",
        "        return chronology\n",
        "\n",
        "    def averages(self):\n",
        "        averages = {}\n",
        "        averages[\"Average Song Length\"] = sum(self.history[\"Duration\"]) / len(self.history[\"Duration\"])\n",
        "        averages[\"Average Song Length Unique\"] = sum(self.songs[\"Duration\"]) / len(self.songs[\"Duration\"])\n",
        "        years = []\n",
        "        for i in self.history[\"Year\"]:\n",
        "            years.append(i[5:10])\n",
        "        averages[\"Average Seconds per Day\"] = sum(self.history[\"Duration\"]) / len(collections.Counter(years))\n",
        "\n",
        "        min_song_length = min(self.songs[\"Duration\"])\n",
        "        max_song_length = max(self.songs[\"Duration\"])\n",
        "        min_song_idx = list(self.songs[\"Duration\"]).index(min_song_length)\n",
        "        max_song_idx = list(self.songs[\"Duration\"]).index(max_song_length)\n",
        "\n",
        "        averages[\"Shortest Song\"] = [min_song_length, self.songs[\"Title\"][min_song_idx], self.songs[\"Artist\"][min_song_idx]]\n",
        "        averages[\"Longest Song\"] = [max_song_idx, self.songs[\"Title\"][max_song_idx], self.songs[\"Artist\"][max_song_idx]]\n",
        "\n",
        "        # 5th percentile song by duration?\n",
        "        # 95th percentile song by duration?\n",
        "        # median song length\n",
        "\n",
        "        history_duration_sorted = self.history[\"Duration\"].copy()\n",
        "        songs_duration_sorted = self.songs[\"Duration\"].copy()\n",
        "        history_duration_sorted = list(history_duration_sorted)\n",
        "        songs_duration_sorted = list(songs_duration_sorted)\n",
        "        history_duration_sorted.sort()\n",
        "        songs_duration_sorted.sort()\n",
        "        averages[\"Median Song Length\"] = history_duration_sorted[int(len(history_duration_sorted)/2)]\n",
        "        averages[\"Median Song Length Unique\"] = songs_duration_sorted[int(len(songs_duration_sorted)/2)]\n",
        "\n",
        "        #plt.hist(list(self.songs[\"Duration\"]), 30, (0, 600))\n",
        "        #plt.show()\n",
        "\n",
        "        averages[\"Average Replays\"] = sum(self.songs[\"Occurrences\"]) / len(self.songs[\"Occurrences\"])\n",
        "        averages[\"Max Replays\"] = max(self.songs[\"Occurrences\"]) # yes this statistic is already calculated somewhere else\n",
        "\n",
        "        frqtable = collections.Counter(self.songs[\"Occurrences\"])\n",
        "\n",
        "        averages[\"Occurrences Frequency Table\"] = frqtable\n",
        "\n",
        "        return averages\n",
        "\n",
        "    def sessions(self):\n",
        "        # split history into sessions (need to decide what distinguishes 1 session from another, most likely 30mins)\n",
        "        # get longest session as a statistic\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution code"
      ],
      "metadata": {
        "id": "WgMjJSI48dMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/My Drive/Colab Notebooks/ytmwrapped\")"
      ],
      "metadata": {
        "id": "f_G-o8b3Q839"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loader.merge_jsons(\"watch-history-new.json\", \"watch-history.json\", \"watch-history-merged.json\")"
      ],
      "metadata": {
        "id": "uul91mtOJiO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loader code\n",
        "apikey = open(\"apitoken.txt\", \"r\").read()\n",
        "filepath = \"watch-history-merged.json\"\n",
        "loadfp = os.getcwd()+\"/generated-reports/22.12.2023-20.13.01\"\n",
        "\n",
        "ignore = {}\n",
        "ignore[\"Title\"] = open(\"ignore-title.txt\", \"r\").read().split(\",\")\n",
        "ignore[\"Artist\"] = open(\"ignore-artist.txt\", \"r\").read().split(\",\")\n",
        "ignore[\"URL\"] = open(\"ignore-url.txt\", \"r\").read().split(\",\")\n",
        "\n",
        "loader = Loader(use_duration=True, analyze_years=[\"2023\"], apikey=apikey, filepath=filepath, loadfp=loadfp, ignores=ignore)\n",
        "history, artists, songs = loader.outs()"
      ],
      "metadata": {
        "id": "HffdIbELSzka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = Analyzer(history, artists, songs)\n",
        "basic, tops, uniques, repeats = analyzer.basic(), analyzer.tops(), analyzer.uniques(), analyzer.repeats()\n",
        "print(\"BASIC\")\n",
        "for i, j in basic.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"TOPS\")\n",
        "for i, j in tops.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"UNIQUES\")\n",
        "for i, j in uniques.items():\n",
        "    print(i)\n",
        "    print(j)\n",
        "print(\"REPEATS\")\n",
        "for i, j in repeats.items():\n",
        "    print(i)\n",
        "    print(j)"
      ],
      "metadata": {
        "id": "7Tm63id7JECV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BROKENS\n",
        "print(\" - Chronology - \")\n",
        "chrono = analyzer.chronology(n=5)\n",
        "for j, i in enumerate(chrono[\"Top n Songs Per Month\"]):\n",
        "    print(j)\n",
        "    print(i)\n",
        "    pass\n",
        "print(list(chrono.items())[1])\n",
        "print(list(chrono.items())[3])\n",
        "print(\" - Averages - \")\n",
        "averages = analyzer.averages()\n",
        "print(averages)\n",
        "\n",
        "print(\"All done!\")"
      ],
      "metadata": {
        "id": "6Q3y3zJjOK5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# songs per times of the day\n",
        "\n",
        "times = chrono[\"Songs Per Time of Day\"]\n",
        "x = sorted(times.keys())\n",
        "y = []\n",
        "for i in x:\n",
        "    y.append(times[i])\n",
        "\n",
        "x = list(reversed(x))\n",
        "y = list(reversed(y))\n",
        "\n",
        "def rightRotate(lists, num):\n",
        "    output_list = []\n",
        "\n",
        "    for item in range(len(lists) - num, len(lists)):\n",
        "        output_list.append(lists[item])\n",
        "\n",
        "    for item in range(0, len(lists) - num):\n",
        "        output_list.append(lists[item])\n",
        "\n",
        "    return output_list\n",
        "\n",
        "x = rightRotate(x, 6)\n",
        "y = rightRotate(y, 6)\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "plt.axis('off')\n",
        "\n",
        "upperLimit = max(y)\n",
        "lowerLimit = min(y)\n",
        "\n",
        "maxval = max(y)\n",
        "\n",
        "slope = (maxval - lowerLimit) / maxval\n",
        "heights = [slope * i + lowerLimit for i in y]\n",
        "\n",
        "width = 2*np.pi / 24\n",
        "\n",
        "indexes = list(range(1, 25))\n",
        "angles = [element * width for element in indexes]\n",
        "\n",
        "grey_heights = [slope*maxval + lowerLimit] * 24\n",
        "\n",
        "# Draw bars\n",
        "bars = ax.bar(\n",
        "    x=angles,\n",
        "    height=grey_heights,\n",
        "    width=width,\n",
        "    bottom=lowerLimit,\n",
        "    linewidth=2,\n",
        "    edgecolor=\"white\",\n",
        "    color=\"#d3d3d3\",\n",
        ")\n",
        "\n",
        "bars = ax.bar(\n",
        "    x=angles,\n",
        "    height=heights,\n",
        "    width=width,\n",
        "    bottom=lowerLimit,\n",
        "    linewidth=2,\n",
        "    edgecolor=\"white\",\n",
        "    color=\"#61a4b2\",\n",
        ")\n",
        "\n",
        "labelPadding = 5\n",
        "\n",
        "for bar, angle, height, label in zip(bars,angles, heights, [str(i) for i in x]):\n",
        "\n",
        "    rotation = np.rad2deg(angle)\n",
        "\n",
        "    alignment = \"\"\n",
        "    if angle >= np.pi/2 and angle < 3*np.pi/2:\n",
        "        alignment = \"right\"\n",
        "        rotation = rotation + 180\n",
        "    else:\n",
        "        alignment = \"left\"\n",
        "\n",
        "    if (height > 100):\n",
        "        ax.text(\n",
        "            x=angle,\n",
        "            y=lowerLimit + height + labelPadding,\n",
        "            s=label,\n",
        "            ha=alignment,\n",
        "            va='center',\n",
        "            rotation=rotation,\n",
        "            rotation_mode=\"anchor\")"
      ],
      "metadata": {
        "id": "Ad--FGUdjKg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listens per day of the week\n",
        "weekdays = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"]\n",
        "plt.bar(weekdays, chrono[\"Days of the Week\"].values())\n",
        "print(\"Made\", chrono[\"Days of the Week Errors\"]. \"errors\")"
      ],
      "metadata": {
        "id": "r69b6bzJiWZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listens per month\n",
        "plt.bar(chrono[\"Songs Per Month\"].keys(), chrono[\"Songs Per Month\"].values())"
      ],
      "metadata": {
        "id": "4znkRaF-74qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(averages['Occurrences Frequency Table'].keys(), averages['Occurrences Frequency Table'].values())\n",
        "plt.yscale(\"linear\") # \"log\""
      ],
      "metadata": {
        "id": "dCn-UcuK73_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MusicBrainz API"
      ],
      "metadata": {
        "id": "yxkOzBYWZxea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genres"
      ],
      "metadata": {
        "id": "tRGcRBc28Zpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allowed_vals_f = open('genres.txt', 'r')\n",
        "allowed_vals = []\n",
        "for i in allowed_vals_f:\n",
        "    allowed_vals.append(i.replace(\"\\n\", \"\"))"
      ],
      "metadata": {
        "id": "_dUJP3gJi_1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_tags = {}\n",
        "print(len(artists['Artist']), \"Artists\")\n",
        "for c, i in enumerate(artists['Artist']):\n",
        "    artist_tags[i] = []\n",
        "    response = requests.get(\"https://musicbrainz.org/ws/2/artist/?fmt=json&query=name:\" + urllib.parse.quote(i.strip()))\n",
        "    if c % 20 == 0:\n",
        "        print(\"Got\", c, \"artists\")\n",
        "    try:\n",
        "        tags = response.json()['artists'][0]['tags']\n",
        "        #print(tags)\n",
        "        for ii in tags:\n",
        "            if ii['name'] in allowed_vals:\n",
        "                artist_tags[i].append(ii['name'])\n",
        "    except:\n",
        "        #print(\"No tags for artist: \" + i)\n",
        "        pass"
      ],
      "metadata": {
        "id": "P3ahcMQZZzfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alltags = itertools.chain(*artist_tags.values())\n",
        "counter = collections.Counter(alltags).most_common(10)\n",
        "genres = []\n",
        "counts = []\n",
        "for (i, j) in counter:\n",
        "    genres.append(i)\n",
        "    counts.append(j)\n",
        "plt.pie(counts, labels=genres, autopct=\"%1.1f%%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XcrJBOmFm4JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alltags = []\n",
        "artist_tags = tags2\n",
        "for i in history['Artist']:\n",
        "    if isinstance(artist_tags[i], str):\n",
        "        alltags.append(artist_tags[i])\n",
        "    else:\n",
        "        for ii in artist_tags[i]:\n",
        "            alltags.append(ii)\n",
        "counter = collections.Counter(alltags).most_common(10)\n",
        "genres = []\n",
        "counts = []\n",
        "for (i, j) in counter:\n",
        "    genres.append(i)\n",
        "    counts.append(j)\n",
        "plt.pie(counts, labels=genres, autopct=\"%1.1f%%\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWMMNMxU9C02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in tags2.items():\n",
        "    if j == []:\n",
        "        print(i)\n",
        "        tags2[i] = input()"
      ],
      "metadata": {
        "id": "JHTepiXk-05T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('artist_tags.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(artist_tags))"
      ],
      "metadata": {
        "id": "tacFdinnqpQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('artist_tags.txt', 'r') as convert_file:\n",
        "     artist_tags = ast.literal_eval(convert_file.read())"
      ],
      "metadata": {
        "id": "GzvTIwcp3enG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Year"
      ],
      "metadata": {
        "id": "FTwbhcCw8gj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just getting genre by genre of artist, not release (because release definitely has some issues)\n",
        "arid = {}\n",
        "ardate = {}\n",
        "count = {}\n",
        "for c, i in enumerate(artists['Artist']):\n",
        "    i = i.strip()\n",
        "    if c%40 == 0:\n",
        "        print(\"Got\", c, \"artists\")\n",
        "    try:\n",
        "        response = requests.get(\"https://musicbrainz.org/ws/2/artist/?fmt=json&query=name:\" + urllib.parse.quote(i))\n",
        "        arid[i] = response.json()['artists'][0]['id']\n",
        "        # store the date in the thing\n",
        "        response = requests.get(\"https://musicbrainz.org/ws/2/release-group/?fmt=json&query=arid:\" + arid[i])\n",
        "        frd = int(response.json()['release-groups'][0]['first-release-date'][0:4])\n",
        "        for ii in response.json()['release-groups']:\n",
        "            try:\n",
        "                t = ii['first-release-date']\n",
        "                if int(t[0:4]) < frd:\n",
        "                    frd = int(t[0:4])\n",
        "            except:\n",
        "                continue\n",
        "        ardate[i] = frd\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "IslM3ZPG8iQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = {}\n",
        "for i in history['Artist']:\n",
        "    i = i.strip()\n",
        "    if i not in ardate.keys():\n",
        "        continue\n",
        "    if ardate[i] in count.keys():\n",
        "        count[ardate[i]] += 1\n",
        "    else:\n",
        "        count[ardate[i]] = 1"
      ],
      "metadata": {
        "id": "vpGBeXMbJbfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(count.keys(), count.values())"
      ],
      "metadata": {
        "id": "-f15bXYzCzGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}